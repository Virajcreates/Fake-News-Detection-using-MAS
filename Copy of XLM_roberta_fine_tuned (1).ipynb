{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEAtirGF7sVw",
        "outputId": "d8ad82fe-490f-4c7f-e5a8-f9e66c15704c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\mas_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label distribution:\n",
            " label\n",
            "0    23481\n",
            "1    21417\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from torch.cuda.amp import autocast\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load datasets\n",
        "df_fake = pd.read_csv(r'C:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\Fake.csv')\n",
        "df_true = pd.read_csv(r'C:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\True.csv')\n",
        "\n",
        "# Combine datasets\n",
        "df_fake['label'] = 0  # Label fake news as 0\n",
        "df_true['label'] = 1  # Label true news as 1\n",
        "df = pd.concat([df_fake, df_true], ignore_index=True)\n",
        "\n",
        "# Remove rows with missing values in 'text'\n",
        "df = df.dropna(subset=['text'])\n",
        "\n",
        "# Shuffle the dataset to ensure proper mixing\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Check if the dataset is balanced\n",
        "print(\"Label distribution:\\n\", df['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7POs95RqfFo",
        "outputId": "b69052e0-36a4-4508-9459-a5cdeab906ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYUDLI0S8I1b"
      },
      "outputs": [],
      "source": [
        "# Initialize tokenizer and model\n",
        "MODEL_NAME = \"xlm-roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Tokenize data\n",
        "MAX_LEN = 84  # Maximum sequence length\n",
        "\n",
        "# Split the dataset\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9uxk7Je8TQ0"
      },
      "outputs": [],
      "source": [
        "# Tokenize and prepare datasets\n",
        "def prepare_data(df, tokenizer, max_len):\n",
        "    inputs = tokenizer(list(df['text']),\n",
        "                       padding='max_length',\n",
        "                       truncation=True,\n",
        "                       max_length=max_len,\n",
        "                       return_tensors='pt')\n",
        "    labels = torch.tensor(df['label'].tolist())\n",
        "    return TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
        "\n",
        "train_data = prepare_data(train_df, tokenizer, MAX_LEN)\n",
        "val_data = prepare_data(val_df, tokenizer, MAX_LEN)\n",
        "test_data = prepare_data(test_df, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPFAuFN6WtlW",
        "outputId": "edff7a9a-0cba-44cc-f21f-3182f4dd1b62"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "c:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\mas_env\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Create DataLoaders\n",
        "BATCH_SIZE = 16\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
        "test_dataloader = DataLoader(test_data, batch_size=1)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=train_df['label'])\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "f354fd58cc5241688cd1d0a4e64b3d6e",
            "e6e2217fee23411fab8e757ab2be3f71",
            "98f8db3db2c948d89034b817d7c17586",
            "dab03ad22d0f4e8b9222d781f5879fb1",
            "03f2674eec53478dbd151aede10ff5cd",
            "fb2e5b3cce9b4b0bb4845ce53e221ade",
            "3d8b59ef6f6f47e4b56138022a0cd6d8",
            "e5b529666dde458285532a25d5ac35f9",
            "600a7c6eac7044d0b73ca46560d1bf4e",
            "14907c79568a42829d58c4456394b54d",
            "2d5f169219d44926b99b424a08319632"
          ]
        },
        "id": "Gnl_JXDS8Y-D",
        "outputId": "fed9098b-4fba-4198-c5f2-cebf9a984680"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f354fd58cc5241688cd1d0a4e64b3d6e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoModelForSequenceClassification, AdamW, get_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Assuming train_data, val_data, train_df, MODEL_NAME, tokenizer are already defined\n",
        "\n",
        "# Create DataLoaders\n",
        "BATCH_SIZE = 16\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
        "test_dataloader = DataLoader(test_data, batch_size=1)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=train_df['label'])\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Track metrics\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        inputs, attention_masks, labels = [t.to(device) for t in batch]\n",
        "\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs, attention_mask=attention_masks)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(logits, labels)\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update model parameters\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "    avg_train_loss = epoch_loss / len(train_dataloader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            inputs, attention_masks, labels = [t.to(device) for t in batch]\n",
        "\n",
        "            outputs = model(inputs, attention_mask=attention_masks)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(logits, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            correct_predictions += (predictions == labels).sum().item()\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    val_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss}, Val Loss = {avg_val_loss}, Val Accuracy = {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Plotting\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Loss graph\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label=\"Train Loss\", marker='o')\n",
        "plt.plot(val_losses, label=\"Validation Loss\", marker='o')\n",
        "plt.title(\"Loss Over Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Accuracy graph\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(val_accuracies, label=\"Validation Accuracy\", marker='o', color='g')\n",
        "plt.title(\"Validation Accuracy Over Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "model.save_pretrained('./fine_tuned_xlm_roberta112')\n",
        "tokenizer.save_pretrained('./fine_tuned_xlm_roberta112')\n",
        "print(\"Model and tokenizer saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ypP2sF6L8vQB",
        "outputId": "de12475d-f590-459f-b1b4-0aba22ac25ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\viraj\\AppData\\Local\\Temp\\ipykernel_21092\\2076366236.py:10: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "c:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\mas_env\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:371: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.4800\n",
            "Test Precision: 0.4800\n",
            "Test Recall: 1.0000\n",
            "Test F1 Score: 0.6486\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Test loop\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        inputs, attention_masks, labels = [t.to(device) for t in batch]\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(inputs, attention_mask=attention_masks)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "        all_predictions.extend(predictions.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Test metrics\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Test F1 Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-edLAAXItWcG",
        "outputId": "c81c653f-98e6-44bc-a986-798259ade440"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The news article is: Real\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Set the device (GPU if available, otherwise CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the fine-tuned XLM-RoBERTa model and tokenizer\n",
        "MODEL_PATH = r'C:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\fine_tuned_xlm_roberta'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
        "\n",
        "# Move the model to the correct device (GPU or CPU)\n",
        "model.to(device)\n",
        "\n",
        "# Function to predict if the news is real or fake\n",
        "def predict(text):\n",
        "    # Tokenize and encode the input text\n",
        "    inputs = tokenizer.encode(text, return_tensors='pt', max_length=512, truncation=True)  # Specify max_length and truncation\n",
        "\n",
        "    # Move the inputs to the same device as the model\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    # Forward pass to get logits\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)\n",
        "        prediction = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "    return 'Real' if prediction.item() == 1 else 'Fake'\n",
        "\n",
        "\n",
        "# Example news article snippet\n",
        "news_article = ('MEXICO CITY (Reuters) - Mexicoâ€™s finance ministry will evaluate whether to make fiscal changes in response to the U.S. tax reform, according to a document seen by Reuters on Friday. In the document, the ministry said Mexico would not make changes that left it with a higher public sector deficit. â€œNevertheless, there will be an assessment of whether modifications should be made to Mexicoâ€™s fiscal framework,â€ the document said')\n",
        "\n",
        "# Predict\n",
        "result = predict(news_article)\n",
        "print(f'The news article is: {result}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykNQRP4ReYIl"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class Agent1:\n",
        "    def __init__(self, case_weight=5, decay_rate=0.1):\n",
        "        self.case_weight = case_weight\n",
        "        self.decay_rate = decay_rate\n",
        "        self.special_symbols = \"!@#$%^&*()_+=-[]{}|;:'\\\",<>?/\\\\\"  # Special symbols\n",
        "\n",
        "    def check_special_symbols(self, text):\n",
        "        if not text:  # Handle empty text case\n",
        "            print(\"Empty text detected.\")\n",
        "            return 0  # No weight allocated for empty text\n",
        "\n",
        "        special_count = 0\n",
        "        total_count = len(text)  # Total number of characters in the text\n",
        "\n",
        "        for char in text:\n",
        "            if char in self.special_symbols:\n",
        "                special_count += 1\n",
        "\n",
        "        # Calculate the percentage of special symbols\n",
        "        percentage = (special_count / total_count) * 100\n",
        "\n",
        "        print(f\"Special Symbols: {special_count}, Total Characters: {total_count}, Percentage: {percentage:.2f}%\")\n",
        "\n",
        "        # Apply weight logic\n",
        "        if percentage <= 20:\n",
        "            weight = self.case_weight\n",
        "        else:\n",
        "            # Exponential decay for percentages above 20%\n",
        "            reduction_factor = math.exp(-self.decay_rate * (percentage - 20))\n",
        "            weight = self.case_weight * reduction_factor\n",
        "            print(f\"Reduction Factor: {reduction_factor:.4f}, Adjusted Weight: {weight:.2f}\")\n",
        "\n",
        "        return weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJqHrxIf4lOj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "class Agent2:\n",
        "    def __init__(self, case_weight=15):\n",
        "        self.case_weight = case_weight\n",
        "        self.analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "    def analyze_sentiment(self, text):\n",
        "        if not text:  # Handle empty text case\n",
        "            print(\"Empty text detected.\")\n",
        "            return 0  # No sentiment detected for empty text\n",
        "\n",
        "        # Analyze the sentiment of the provided text\n",
        "        sentiment_score = self.analyzer.polarity_scores(text)\n",
        "        compound_score = sentiment_score['compound']\n",
        "\n",
        "\n",
        "\n",
        "        # Calculate percentage based on compound score\n",
        "        if compound_score == 0:\n",
        "            # Neutral sentiment\n",
        "            sentiment_percentage = 50\n",
        "        elif compound_score > 0:\n",
        "            # Positive sentiment\n",
        "            sentiment_percentage = 50 + (compound_score * 50)  # Linear increase from 50% to 100%\n",
        "        else:\n",
        "            # Negative sentiment\n",
        "            sentiment_percentage = 50 + (compound_score * 50)  # Linear decrease from 50% to 0%\n",
        "\n",
        "        print(f\"Sentiment Percentage: {sentiment_percentage:.2f}%\")\n",
        "\n",
        "        # Final weight calculation based on sentiment percentage\n",
        "        final_weight = (self.case_weight * sentiment_percentage) / 100\n",
        "        return final_weight\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFX-zFHA4qLn"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class Agent3:\n",
        "    def __init__(self, case_weight=5, threshold=10, decay_rate=0.025):\n",
        "        \"\"\"\n",
        "        Initialize Agent3 with parameters for weight, threshold, and decay rate.\n",
        "\n",
        "        Args:\n",
        "            case_weight (float): The base weight to assign.\n",
        "            threshold (float): The percentage threshold for applying decay.\n",
        "            decay_rate (float): The rate of exponential decay for weights above the threshold.\n",
        "        \"\"\"\n",
        "        self.case_weight = case_weight\n",
        "        self.threshold = threshold\n",
        "        self.decay_rate = decay_rate\n",
        "\n",
        "    def count_hashtags(self, text):\n",
        "        \"\"\"\n",
        "        Count the percentage of words in the text that are hashtags and apply weight logic.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text to analyze.\n",
        "\n",
        "        Returns:\n",
        "            float: The adjusted weight based on hashtag percentage and decay logic.\n",
        "        \"\"\"\n",
        "        if not text:  # Handle empty text case\n",
        "            print(\"Empty text detected.\")\n",
        "            return 0.0  # No hashtags detected for empty text\n",
        "\n",
        "        # Split the text into words to calculate the percentage of hashtags\n",
        "        words = text.split()\n",
        "        hashtag_count = sum(1 for word in words if word.startswith('#'))\n",
        "\n",
        "        # Calculate the percentage of hashtags relative to the total number of words\n",
        "        total_words = len(words)\n",
        "\n",
        "        if total_words == 0:  # Avoid division by zero\n",
        "            return 0.0\n",
        "\n",
        "        hashtag_percentage = (hashtag_count / total_words) * 100\n",
        "\n",
        "        print(f\"Hashtags: {hashtag_count}, Total Words: {total_words}, Percentage: {hashtag_percentage:.2f}%\")\n",
        "\n",
        "        # Apply weight logic based on the threshold\n",
        "        if hashtag_percentage <= self.threshold:\n",
        "            weight = self.case_weight\n",
        "            print(f\"Hashtag Percentage below threshold. Weight: {weight}\")\n",
        "        else:\n",
        "            # Exponential decay for percentages above the threshold\n",
        "            reduction_factor = math.exp(-self.decay_rate * (hashtag_percentage - self.threshold))\n",
        "            weight = self.case_weight * reduction_factor\n",
        "            print(f\"Reduction Factor: {reduction_factor:.4f}, Adjusted Weight: {weight:.2f}\")\n",
        "\n",
        "        return weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7izM4shO43K5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import math\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "class Agent4:\n",
        "    def __init__(self, case_weight=25, decay_rate=0.1, threshold=10):\n",
        "        self.case_weight = case_weight  # Base weight for the agent\n",
        "        self.decay_rate = decay_rate   # How quickly the weight reduces above the threshold\n",
        "        self.threshold = threshold    # Percentage threshold for decay application\n",
        "        self.analyzer = SentimentIntensityAnalyzer()\n",
        "        self.clickbait_keywords = [\n",
        "            \"shocking\", \"you won’t believe\", \"what happened next\", \"this is why\",\n",
        "            \"never seen before\", \"mind-blowing\", \"amazing\", \"unbelievable\",\n",
        "            \"click here\", \"must read\", \"you'll regret not\", \"read now\", \"only\",\n",
        "            \"warning\", \"secret\", \"revealed\", \"discovered\"\n",
        "        ]\n",
        "\n",
        "    def check_clickbait(self, text):\n",
        "        if not text:  # Handle empty text case\n",
        "            print(\"Empty text detected.\")\n",
        "            return 0  # No weight allocated for empty text\n",
        "\n",
        "        # Lowercase for uniformity\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Check for clickbait keywords\n",
        "        keyword_count = sum(1 for keyword in self.clickbait_keywords if keyword in text_lower)\n",
        "\n",
        "        # Check for excessive punctuation\n",
        "        exclamation_marks = len(re.findall(r'!', text))\n",
        "        question_marks = len(re.findall(r'\\?', text))\n",
        "        punctuation_score = 1 if exclamation_marks > 2 or question_marks > 1 else 0\n",
        "\n",
        "        # Check sentiment score\n",
        "        sentiment_score = self.analyzer.polarity_scores(text)['compound']\n",
        "        sentiment_clickbait = 1 if sentiment_score <= -0.5 else 0\n",
        "\n",
        "        print(f\"Keyword Count: {keyword_count}\")\n",
        "        print(f\"Exclamation Marks: {exclamation_marks}, Question Marks: {question_marks}\")\n",
        "        print(f\"Punctuation Score: {punctuation_score}\")\n",
        "        print(f\"Sentiment Score: {sentiment_score}, Sentiment Clickbait: {sentiment_clickbait}\")\n",
        "\n",
        "\n",
        "\n",
        "        # Combine results into a \"clickbait score\"\n",
        "        clickbait_score = keyword_count + punctuation_score + sentiment_clickbait\n",
        "\n",
        "        # Set theoretical maximum score (4, if all factors are contributing)\n",
        "        max_possible_score = 4  # Maximum score for the clickbait components considered\n",
        "\n",
        "        # Calculate clickbait percentage\n",
        "        if max_possible_score == 0:  # Edge case if max_possible_score is zero\n",
        "            clickbait_percentage = 0\n",
        "        else:\n",
        "            clickbait_percentage = (clickbait_score / max_possible_score) * 100\n",
        "\n",
        "        print(f\"Clickbait Score: {clickbait_score}, Percentage: {clickbait_percentage:.2f}%\")\n",
        "\n",
        "        # Apply weight logic based on clickbait percentage\n",
        "        if clickbait_percentage <= self.threshold:\n",
        "            weight = self.case_weight\n",
        "        else:\n",
        "            # Apply exponential decay for percentages above the threshold\n",
        "            reduction_factor = math.exp(-self.decay_rate * (clickbait_percentage - self.threshold))\n",
        "            weight = self.case_weight * reduction_factor\n",
        "            print(f\"Reduction Factor: {reduction_factor:.4f}, Adjusted Weight: {weight:.2f}\")\n",
        "\n",
        "        return weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGK5Qopr46SE"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class Agent5:\n",
        "    def __init__(self, model_path=r'C:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\fine_tuned_xlm_roberta', max_length=256, true_weight=45):\n",
        "        # Load the pre-trained tokenizer and model from the given path\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "        self.max_length = max_length\n",
        "        self.true_weight = true_weight  # Assign only the true weight during initialization\n",
        "\n",
        "    def train(self, train_texts, train_labels, batch_size=8, epochs=3):\n",
        "        # Convert text data into a Dataset\n",
        "        train_dataset = NewsDataset(train_texts, train_labels, self.tokenizer, self.max_length)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "\n",
        "        # Use the AdamW optimizer and cross-entropy loss\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5)\n",
        "        loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        # Move model to GPU if available\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(device)\n",
        "\n",
        "        # Training loop\n",
        "        self.model.train()\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for batch in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Move data to the same device as model\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                loss.backward()\n",
        "\n",
        "                # Optimizer step\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f'Epoch {epoch + 1}/{epochs} - Loss: {running_loss / len(train_loader)}')\n",
        "\n",
        "    def predict_true_weight(self, text):\n",
        "        # Tokenize and prepare input text\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids']\n",
        "        attention_mask = encoding['attention_mask']\n",
        "\n",
        "        # Move data to the same device as model\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(device)\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "\n",
        "        # Prediction\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Extract true news probability and calculate weight\n",
        "        true_prob = probs[0][1].item() * 100  # Probability of being true\n",
        "        true_weighted = true_prob * self.true_weight / 100  # Weighted true value\n",
        "        print(f\"True Probability: {true_prob:.2f}%, Weighted True Value: {true_weighted:.2f}\")\n",
        "\n",
        "        return true_prob\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EROQwt7p4_oC"
      },
      "outputs": [],
      "source": [
        "import tweepy\n",
        "\n",
        "class Agent7:\n",
        "    def __init__(self, bearer_token):\n",
        "        \"\"\"\n",
        "        Initializes Agent7 with the necessary tools for tweet verification.\n",
        "\n",
        "        :param bearer_token: Twitter API bearer token for authentication.\n",
        "        \"\"\"\n",
        "        self.tweepy = tweepy  # Store tweepy module as a class attribute\n",
        "        self.client = tweepy.Client(bearer_token=bearer_token)\n",
        "\n",
        "    def check_tweet_existence(self, query, max_results=10):\n",
        "        \"\"\"\n",
        "        Checks if any tweets exist for a given query.\n",
        "\n",
        "        :param query: Search query string (e.g., \"AI technology\").\n",
        "        :param max_results: Maximum number of tweets to fetch for verification.\n",
        "        :return: Tuple (return_code, tweets).\n",
        "        \"\"\"\n",
        "        search_query = f\"{query} -is:retweet lang:en\"  # Filter retweets, only English\n",
        "        try:\n",
        "            response = self.client.search_recent_tweets(query=search_query, max_results=max_results)\n",
        "            if response.data:\n",
        "                tweets = [tweet.text for tweet in response.data]\n",
        "                return 0, tweets  # Return 0 when tweets are found\n",
        "            else:\n",
        "                return 1, []  # Return 1 when no tweets are found\n",
        "        except self.tweepy.errors.TooManyRequests:\n",
        "            print(\"Error: Rate limit exceeded. Please try again later.\")\n",
        "            return 2, []  # Return 2 when rate limit error occurs\n",
        "        except self.tweepy.errors.Unauthorized:\n",
        "            print(\"Error: Unauthorized access. Check your bearer token.\")\n",
        "            return 1, []  # Return 1 for unauthorized access (no tweets)\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error while checking tweet existence: {e}\")\n",
        "            return 1, []  # Return 1 for any other errors (no tweets)\n",
        "\n",
        "    def fetch_and_check(self, query, max_results=10):\n",
        "        \"\"\"\n",
        "        Combines fetching and verifying tweets for a given query.\n",
        "\n",
        "        :param query: Search query string.\n",
        "        :param max_results: Maximum number of tweets to fetch for verification.\n",
        "        :return: Tuple (return_code, tweets).\n",
        "        \"\"\"\n",
        "        # Call the check_tweet_existence function\n",
        "        return self.check_tweet_existence(query, max_results)\n",
        "\n",
        "\n",
        "        # Allocate dynamic weight or handle scraping issue\n",
        "        if return_code == 0:\n",
        "            return 10  # Return 10 when tweets are found\n",
        "        elif return_code == 1:\n",
        "            return 0  # Return 0 when no tweets are found\n",
        "        elif return_code == 2:\n",
        "            return \"Not Working\"  # Return \"Not Working\" for rate limit or errors\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnWBlpdH9Yo2",
        "outputId": "c352ea2f-06f0-4a10-f252-69396bb9c703"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Agents...\n",
            "Special Symbols: 70, Total Characters: 2699, Percentage: 2.59%\n",
            "Agent 1 Output: 5\n",
            "Sentiment Percentage: 99.04%\n",
            "Agent 2 Output (Sentiment): 14.856\n",
            "Hashtags: 2, Total Words: 426, Percentage: 0.47%\n",
            "Hashtag Percentage below threshold. Weight: 5\n",
            "Agent 3 Output (Hashtags Percentage): 5\n",
            "Keyword Count: 0\n",
            "Exclamation Marks: 5, Question Marks: 4\n",
            "Punctuation Score: 1\n",
            "Sentiment Score: 0.9808, Sentiment Clickbait: 0\n",
            "Clickbait Score: 1, Percentage: 25.00%\n",
            "Reduction Factor: 0.2231, Adjusted Weight: 5.58\n",
            "Agent 4 Output (Clickbait): 5.578254003710746\n",
            "True Probability: 0.00%, Weighted True Value: 0.00\n",
            "Agent 5 (Fake News Prediction): Real Probability: 0.003167155955452472%, Fake Probability: 99.99683284404455%\n",
            "Unexpected error while checking tweet existence: HTTPSConnectionPool(host='api.twitter.com', port=443): Max retries exceeded with url: /2/tweets/search/recent?max_results=10&query=None+-is%3Aretweet+lang%3Aen (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000232C6AA90A0>: Failed to resolve 'api.twitter.com' ([Errno 11001] getaddrinfo failed)\"))\n",
            "Agent 7 Output (Tweet Result Code): 1, Tweets: []\n",
            "Applying Weights...\n",
            "Individual Weight Contributions:\n",
            "Agent1 Weight Contribution: 5.00000\n",
            "Agent2 Weight Contribution: 14.85600\n",
            "Agent3 Weight Contribution: 5.00000\n",
            "Agent4 Weight Contribution: 5.57825\n",
            "Agent5 Weight Contribution: 0.00127\n",
            "Agent7 Weight Contribution: 10.00000\n",
            "\n",
            "Total Weight Contribution from all Agents: 40.44\n",
            "Final Weighted Score: 40.44\n",
            "Final Weighted Score: 40.44\n"
          ]
        }
      ],
      "source": [
        "class Agent6Integration:\n",
        "    def __init__(self, agent1, agent2, agent3, agent4, agent5, agent7):  # Correct constructor name\n",
        "        self.agent1 = agent1\n",
        "        self.agent2 = agent2\n",
        "        self.agent3 = agent3\n",
        "        self.agent4 = agent4\n",
        "        self.agent5 = agent5\n",
        "        self.agent7 = agent7\n",
        "\n",
        "    def run_agents(self, text, query=None, max_results=10, headline=None, body=None):\n",
        "        print(\"Running Agents...\")  # Debugging statement\n",
        "\n",
        "        # Run Agent 1 (Special symbols check)\n",
        "        special_symbols_percent = self.agent1.check_special_symbols(text)\n",
        "        print(f\"Agent 1 Output: {special_symbols_percent}\")  # Debugging statement\n",
        "\n",
        "        # Run Agent 2 (Sentiment analysis)\n",
        "        sentiment = self.agent2.analyze_sentiment(text)\n",
        "        print(f\"Agent 2 Output (Sentiment): {sentiment}\")  # Debugging statement\n",
        "\n",
        "        # Run Agent 3 (Hashtags count)\n",
        "        hashtags_percent = self.agent3.count_hashtags(text)\n",
        "        print(f\"Agent 3 Output (Hashtags Percentage): {hashtags_percent}\")  # Debugging statement\n",
        "\n",
        "        # Run Agent 4 (Clickbait detection)\n",
        "        clickbait = self.agent4.check_clickbait(text)\n",
        "        print(f\"Agent 4 Output (Clickbait): {clickbait}\")  # Debugging statement\n",
        "\n",
        "        # Run Agent 5 (Fake news prediction using BERT)\n",
        "        real_prob = self.agent5.predict_true_weight(text)\n",
        "        fake_prob = 100 - real_prob  # Debugging statement\n",
        "        print(f\"Agent 5 (Fake News Prediction): Real Probability: {real_prob}%, Fake Probability: {fake_prob}%\")\n",
        "\n",
        "        # Run Agent 7 (Tweet existence check)\n",
        "        tweet_result_code, tweets = self.agent7.fetch_and_check(query)\n",
        "        print(f\"Agent 7 Output (Tweet Result Code): {tweet_result_code}, Tweets: {tweets}\")  # Debugging statement\n",
        "\n",
        "        # Create the standardized output dictionary\n",
        "        standardized_outputs = {\n",
        "            \"Agent1_Percentage\": special_symbols_percent,\n",
        "            \"Agent2_Binary\":sentiment,\n",
        "            \"Agent3_Percentage\": hashtags_percent,\n",
        "            \"Agent4_Binary\": clickbait,\n",
        "            \"Agent5_Percentage\": real_prob,\n",
        "            \"Agent7_Categorical\": tweet_result_code,\n",
        "        }\n",
        "\n",
        "        # Now use Agent6 to apply weights\n",
        "        agent6 = Agent6()\n",
        "        weighted_score = agent6.apply_weights(standardized_outputs)\n",
        "\n",
        "        return weighted_score\n",
        "\n",
        "\n",
        "class Agent6:\n",
        "    def __init__(self):  # Correct constructor name\n",
        "        pass\n",
        "\n",
        "    def apply_weights(self, standardized_outputs):\n",
        "        print(\"Applying Weights...\")  # Debugging statement\n",
        "        # Retrieve the output from Agent 7\n",
        "        agent7_output = standardized_outputs[\"Agent7_Categorical\"]\n",
        "\n",
        "        # Define weights based on Agent7 output (if Twitter query results in issues)\n",
        "        if agent7_output == 2:  # Rate limit exceeded\n",
        "            weights = {\n",
        "                \"Agent1\": 5,\n",
        "                \"Agent2\": 15,\n",
        "                \"Agent3\": 5,\n",
        "                \"Agent4\": 25,\n",
        "                \"Agent5\": 45,\n",
        "                \"Agent7\": 0,\n",
        "            }\n",
        "        else:\n",
        "            # Assign normal weights if no issues with Agent7\n",
        "            weights = {\n",
        "                \"Agent1\": 5,\n",
        "                \"Agent2\": 15,\n",
        "                \"Agent3\": 5,\n",
        "                \"Agent4\": 20,\n",
        "                \"Agent5\": 40,\n",
        "                \"Agent7\": 10,\n",
        "            }\n",
        "\n",
        "        # Apply weights and calculate the final weighted score\n",
        "        adjusted_weights = {}\n",
        "        total_weight_contribution = 0\n",
        "        print(\"Individual Weight Contributions:\")\n",
        "\n",
        "        for agent, weight in weights.items():\n",
        "            if agent == \"Agent1\":\n",
        "                contribution = (standardized_outputs[\"Agent1_Percentage\"])\n",
        "                adjusted_weights[agent] = contribution\n",
        "            elif agent == \"Agent2\":\n",
        "                contribution = (standardized_outputs[\"Agent2_Binary\"])\n",
        "                adjusted_weights[agent] = contribution\n",
        "            elif agent == \"Agent3\":\n",
        "                contribution =(standardized_outputs[\"Agent3_Percentage\"])\n",
        "                adjusted_weights[agent] = contribution\n",
        "            elif agent == \"Agent4\":\n",
        "                contribution =(standardized_outputs[\"Agent4_Binary\"])\n",
        "                adjusted_weights[agent] = contribution\n",
        "            elif agent == \"Agent5\":\n",
        "                contribution = weight * (standardized_outputs[\"Agent5_Percentage\"]/100)\n",
        "                adjusted_weights[agent] = contribution\n",
        "            elif agent == \"Agent7\":\n",
        "                contribution = weight * (1 if agent7_output in [0, 1] else 0)\n",
        "                adjusted_weights[agent] = contribution\n",
        "\n",
        "            # Add contribution to total weight\n",
        "            total_weight_contribution += contribution\n",
        "\n",
        "            # Print each agent's weight contribution\n",
        "            print(f\"{agent} Weight Contribution: {contribution:.5f}\")\n",
        "\n",
        "        # Display total weight before final weighted score\n",
        "        print(f\"\\nTotal Weight Contribution from all Agents: {total_weight_contribution:.2f}\")\n",
        "\n",
        "        # Calculate the final weighted score\n",
        "        print(f\"Final Weighted Score: {total_weight_contribution:.2f}\")\n",
        "        return total_weight_contribution\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize agents (you'll need to replace these with real implementations or mocks)\n",
        "    agent1 = Agent1()  # Assume you have the Agent1 class already\n",
        "    agent2 = Agent2()  # Assume you have the Agent2 class already\n",
        "    agent3 = Agent3()  # Assume you have the Agent3 class already\n",
        "    agent4 = Agent4()  # Assume you have the Agent4 class already\n",
        "    agent5 = Agent5()  # Assume you have the Agent5 class already\n",
        "    agent7 = Agent7(bearer_token=\"AAAAAAAAAAAAAAAAAAAAADe%2BxAEAAAAA9cGGVbvZCC5bgEi7gC%2FfKPqQWRo%3DMFuNsg5s3WrblEcbfkUAdw88olEzchaehNpAShH7sKpMHbaH3V\")  # Your actual token\n",
        "\n",
        "    # Integrate agents and Agent6\n",
        "    agent6_integration = Agent6Integration(agent1, agent2, agent3, agent4, agent5, agent7)\n",
        "\n",
        "    # Test sentence for classification (user input)\n",
        "    text = input(\"Please enter a sentence for classification: \")\n",
        "\n",
        "    # Optionally, provide additional parameters (query, headline, body)\n",
        "    query = \"KKK Grand Wizard David Duke THANKS Trump For Championing White Supremacy At Unhinged Presser\"\n",
        "    headline = \"The shocking truth about AI\"\n",
        "    body = \"This is a must-read article that unveils the shocking truth about AI and technology.\"\n",
        "\n",
        "    # Run all agents and get the final weighted score\n",
        "    weighted_score = agent6_integration.run_agents(text, max_results=10)\n",
        "    print(f\"Final Weighted Score: {weighted_score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm-npNfw__Zi"
      },
      "outputs": [],
      "source": [
        "\"AAAAAAAAAAAAAAAAAAAAAKZWxgEAAAAAwpP4iiwWC0bs%2FJYsBUy02Rv4a6Q%3D3RPDGotbIOrIvjN2e7J2ogKNRDEmklxsIvLNvxlDvbObUejkgq\"\n",
        "\n",
        "\n",
        "\"AAAAAAAAAAAAAAAAAAAAAD9WxgEAAAAAcN7XpBMYsOwXEmB93%2FBI2%2B8U7gg%3DOzTnEBwoNebqaIPKCOlD1C8QOd1CC2wCkENDHVeaueytPABYUy\"\n",
        "\n",
        "\n",
        "\"AAAAAAAAAAAAAAAAAAAAADe%2BxAEAAAAA9cGGVbvZCC5bgEi7gC%2FfKPqQWRo%3DMFuNsg5s3WrblEcbfkUAdw88olEzchaehNpAShH7sKpMHbaH3V\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mas_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03f2674eec53478dbd151aede10ff5cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14907c79568a42829d58c4456394b54d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d5f169219d44926b99b424a08319632": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d8b59ef6f6f47e4b56138022a0cd6d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "600a7c6eac7044d0b73ca46560d1bf4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98f8db3db2c948d89034b817d7c17586": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5b529666dde458285532a25d5ac35f9",
            "max": 1115567652,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_600a7c6eac7044d0b73ca46560d1bf4e",
            "value": 1115567652
          }
        },
        "dab03ad22d0f4e8b9222d781f5879fb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14907c79568a42829d58c4456394b54d",
            "placeholder": "​",
            "style": "IPY_MODEL_2d5f169219d44926b99b424a08319632",
            "value": " 1.12G/1.12G [00:04&lt;00:00, 223MB/s]"
          }
        },
        "e5b529666dde458285532a25d5ac35f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6e2217fee23411fab8e757ab2be3f71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb2e5b3cce9b4b0bb4845ce53e221ade",
            "placeholder": "​",
            "style": "IPY_MODEL_3d8b59ef6f6f47e4b56138022a0cd6d8",
            "value": "model.safetensors: 100%"
          }
        },
        "f354fd58cc5241688cd1d0a4e64b3d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6e2217fee23411fab8e757ab2be3f71",
              "IPY_MODEL_98f8db3db2c948d89034b817d7c17586",
              "IPY_MODEL_dab03ad22d0f4e8b9222d781f5879fb1"
            ],
            "layout": "IPY_MODEL_03f2674eec53478dbd151aede10ff5cd"
          }
        },
        "fb2e5b3cce9b4b0bb4845ce53e221ade": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}