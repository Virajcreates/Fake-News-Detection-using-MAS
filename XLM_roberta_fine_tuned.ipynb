{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1733,
     "status": "ok",
     "timestamp": 1732105581672,
     "user": {
      "displayName": "VIRAJ D NAIK",
      "userId": "05442403411645043460"
     },
     "user_tz": -330
    },
    "id": "YEAtirGF7sVw",
    "outputId": "33701f8f-e13d-4459-81a1-0048684c2947"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\mas_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution:\n",
      " label\n",
      "0    23481\n",
      "1    21417\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.cuda.amp import autocast\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load datasets\n",
    "df_fake = pd.read_csv(r'C:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\Fake.csv')\n",
    "df_true = pd.read_csv(r'C:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\True.csv')\n",
    "\n",
    "# Combine datasets\n",
    "df_fake['label'] = 0  # Label fake news as 0\n",
    "df_true['label'] = 1  # Label true news as 1\n",
    "df = pd.concat([df_fake, df_true], ignore_index=True)\n",
    "\n",
    "# Remove rows with missing values in 'text'\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# Shuffle the dataset to ensure proper mixing\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Check if the dataset is balanced\n",
    "print(\"Label distribution:\\n\", df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269,
     "referenced_widgets": [
      "a9b97cfc7aec499b9c7938708aef5ced",
      "46b5471f23134224bded2854623fa309",
      "347b4a1815fb4d7eb615ae589d99a79f",
      "9f528776008349548cc089296519e8f3",
      "e2540bb6c983416699b938ed0d90b8a3",
      "8b1ce1baa6834c4bbb43fbefa562cf26",
      "ec316d0f17c2458599af6b7c26e67606",
      "c8fb3cec2b0349ce88efa0674c36ccd1",
      "01b0c8aacead44d08f705bb72df90c06",
      "e13bb378cfcb46cb8596a30b570dd6ff",
      "8a88f40f7f274bdb8a48878d546aa1bc",
      "c371f65583c3476cb789c6d041350b82",
      "05262276ea9049c595136ed72798aaf6",
      "20e82ce495e14c2493f25a44785f5c45",
      "0c293fc53f1a498ea4a1056d6ef52194",
      "d86e662046154edf808f1a50664e6aac",
      "5b3f51b2756c42c1940f01c92764d2b0",
      "4627fb01ec3b49ff83cb193d15928ab5",
      "74084694b54d4329a53b02864e47ae64",
      "ea9c0b763e214b27912f4134a6e774a8",
      "dd53f8d132464f1db09529ecd2e00a3a",
      "1d09ef0161ed43b7bcc79a44a56e5e98",
      "3d49c6409ce04754bf7edaf228d15ed9",
      "b316fd403e52433cb347ab9ed64c3ac7",
      "b86fda9a73694286a1df15afeb95129b",
      "f62b91c58f924015bb84dd9912242a8f",
      "757805568b3c40da83aeef37b72f0f3a",
      "452a570a5681462b99f0fbe1f1698ffb",
      "426c4ff9417743d5b3cef2627ac66f51",
      "e8aa1a2a90d042b6a08db1c097e6b0b1",
      "41ec069c61c94a6798bb61e85816d1a9",
      "b30949ba3fb1413d9a7dce610cfd586e",
      "c6f9831667fa4aab8c5aae2ca4704c16",
      "c303a708f096419b8d0042356f35d1c9",
      "868a9029bfc44520af9c184b46e25d41",
      "46ba3c4b2457480a98ad7bdc7e408b0e",
      "8d5064ceee164e70a0c4747db9c4073f",
      "b3f22b02c4e14846838ee002491f8676",
      "d377aeb89bf44900bb02749f755377ca",
      "b98a4364a965460cbaa9481042f72548",
      "ec74ba0c59da483a8f3168ac372617ca",
      "09fcf47001f5466591ff6bdbcd5eeeb0",
      "942c1ac075794068b907935e95d86d7a",
      "0b2e2e8ccd53448fb72215def338fa34"
     ]
    },
    "executionInfo": {
     "elapsed": 8060,
     "status": "ok",
     "timestamp": 1732105597388,
     "user": {
      "displayName": "VIRAJ D NAIK",
      "userId": "05442403411645043460"
     },
     "user_tz": -330
    },
    "id": "yYUDLI0S8I1b",
    "outputId": "6c74cabd-9587-4f2a-9325-d04b252d96f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\mas_env\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\viraj\\.cache\\huggingface\\hub\\models--xlm-roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and model\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenize data\n",
    "MAX_LEN = 84  # Maximum sequence length\n",
    "\n",
    "# Split the dataset\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "H9uxk7Je8TQ0"
   },
   "outputs": [],
   "source": [
    "# Tokenize and prepare datasets\n",
    "def prepare_data(df, tokenizer, max_len):\n",
    "    inputs = tokenizer(list(df['text']),\n",
    "                       padding='max_length',\n",
    "                       truncation=True,\n",
    "                       max_length=max_len,\n",
    "                       return_tensors='pt')\n",
    "    labels = torch.tensor(df['label'].tolist())\n",
    "    return TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
    "\n",
    "train_data = prepare_data(train_df, tokenizer, MAX_LEN)\n",
    "val_data = prepare_data(val_df, tokenizer, MAX_LEN)\n",
    "test_data = prepare_data(test_df, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\mas_env\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "BATCH_SIZE = 16\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=train_df['label'])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243,
     "referenced_widgets": [
      "ce493ed4ee7c4fb89ea381f4f9bd02d7",
      "6265527751fa4b2c811f27a367349758",
      "4c6bce7efb03445d8fdb0b8db7cd0576",
      "ba4b11280c7f4b68ac1b8076999bd932",
      "aa2371e735a540b2890e2df7900a515f",
      "81e276447034495eab56dd8831389e30",
      "37415394f753457285d2cbeb11c18b27",
      "f6701a3dbdb34e38bb2f4749f90ad267",
      "ce702fccfa454da9aa0f5208f3782b8e",
      "4db5c648c4dd46528203c560d5152c96",
      "8c2ac2adc3fe4b69af9dd973b79b3531"
     ]
    },
    "executionInfo": {
     "elapsed": 65813,
     "status": "ok",
     "timestamp": 1732107681620,
     "user": {
      "displayName": "VIRAJ D NAIK",
      "userId": "05442403411645043460"
     },
     "user_tz": -330
    },
    "id": "Gnl_JXDS8Y-D",
    "outputId": "413e8dfe-8a88-423b-ff2d-53bae7f9ee64"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce493ed4ee7c4fb89ea381f4f9bd02d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████| 2021/2021 [10:36<00:00,  3.17it/s, loss=0.000269]\n",
      "Epoch 2: 100%|██████████| 2021/2021 [10:40<00:00,  3.15it/s, loss=6.18e-5]\n",
      "Epoch 3: 100%|██████████| 2021/2021 [10:39<00:00,  3.16it/s, loss=6.67e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.00010715113910717062\n",
      "Validation Accuracy: 100.00%\n",
      "Model and tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "BATCH_SIZE = 16\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=train_df['label'])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        inputs, attention_masks, labels = [t.to(device) for t in batch]\n",
    "\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, attention_mask=attention_masks)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "# Validation loop\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        inputs, attention_masks, labels = [t.to(device) for t in batch]\n",
    "\n",
    "        outputs = model(inputs, attention_mask=attention_masks)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "# Validation metrics\n",
    "avg_val_loss = val_loss / len(val_dataloader)\n",
    "val_accuracy = correct_predictions / total_predictions\n",
    "print(f\"Validation Loss: {avg_val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained('./fine_tuned_xlm_roberta')\n",
    "tokenizer.save_pretrained('./fine_tuned_xlm_roberta')\n",
    "print(\"Model and tokenizer saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 164224,
     "status": "ok",
     "timestamp": 1732108610391,
     "user": {
      "displayName": "VIRAJ D NAIK",
      "userId": "05442403411645043460"
     },
     "user_tz": -330
    },
    "id": "ypP2sF6L8vQB",
    "outputId": "aa233147-95f7-48a8-cff6-5cd7520f97fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viraj\\AppData\\Local\\Temp\\ipykernel_19788\\762266515.py:10: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "c:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\mas_env\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:371: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5175\n",
      "Test Precision: 0.0000\n",
      "Test Recall: 0.0000\n",
      "Test F1 Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\mas_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Test loop\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        inputs, attention_masks, labels = [t.to(device) for t in batch]\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(inputs, attention_mask=attention_masks)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Test metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5075,
     "status": "ok",
     "timestamp": 1732267671794,
     "user": {
      "displayName": "VIRAJ D NAIK",
      "userId": "05442403411645043460"
     },
     "user_tz": -330
    },
    "id": "-edLAAXItWcG",
    "outputId": "af0bcbe3-3370-4fe6-c43b-fc7222cde33d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The news article is: Fake\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Set the device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the fine-tuned XLM-RoBERTa model and tokenizer\n",
    "MODEL_PATH = r'C:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\fine_tuned_xlm_roberta'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Move the model to the correct device (GPU or CPU)\n",
    "model.to(device)\n",
    "\n",
    "# Function to predict if the news is real or fake\n",
    "def predict(text):\n",
    "    # Tokenize and encode the input text\n",
    "    inputs = tokenizer.encode(text, return_tensors='pt', max_length=512, truncation=True)  # Specify max_length and truncation\n",
    "\n",
    "    # Move the inputs to the same device as the model\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Forward pass to get logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        prediction = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "    return 'Real' if prediction.item() == 1 else 'Fake'\n",
    "\n",
    "\n",
    "# Example news article snippet\n",
    "news_article = ('who was')\n",
    "\n",
    "# Predict\n",
    "result = predict(news_article)\n",
    "print(f'The news article is: {result}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6626,
     "status": "ok",
     "timestamp": 1732268388160,
     "user": {
      "displayName": "VIRAJ D NAIK",
      "userId": "05442403411645043460"
     },
     "user_tz": -330
    },
    "id": "bq6YIS88F6zM",
    "outputId": "76750374-10c1-4c8a-e4d1-7b29ee2ebec1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Loss: 4.666774272918701\n",
      "Epoch 2/2 - Loss: 3.4869813919067383\n",
      "Prediction: Real News\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Custom Dataset Class for loading the input text data\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Agent 5 Class with XLM-RoBERTa\n",
    "class Agent5:\n",
    "    def __init__(self, model_path='/content/drive/MyDrive/fine_tuned_xlm_roberta', max_length=256):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def train(self, train_texts, train_labels, batch_size=8, epochs=3):\n",
    "        # Convert text data into a Dataset\n",
    "        train_dataset = NewsDataset(train_texts, train_labels, self.tokenizer, self.max_length)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "        # Use the AdamW optimizer and cross entropy loss\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # Move model to GPU if available\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Move data to the same device as model\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "\n",
    "                # Optimizer step\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{epochs} - Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "    def predict(self, text):\n",
    "        # Prepare the input text for prediction\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "\n",
    "        # Move data to the same device as model\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        # Prediction\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        return 'Fake News' if predictions.item() == 1 else 'Real News'\n",
    "\n",
    "\n",
    "# Example of using Agent5\n",
    "\n",
    "# Initialize Agent5 with fine-tuned XLM-RoBERTa\n",
    "agent5 = Agent5(model_path='/content/drive/MyDrive/fine_tuned_xlm_roberta')\n",
    "\n",
    "# Sample train data (you should replace this with your actual dataset)\n",
    "train_texts = [\"Breaking news: The stock market crashes!\", \"Local hero saves the day in small town.\"]\n",
    "train_labels = [1, 0]  # 1 for Fake, 0 for Real\n",
    "\n",
    "# Train the model (use proper data here for actual use)\n",
    "agent5.train(train_texts, train_labels, epochs=2)\n",
    "\n",
    "# Example prediction\n",
    "text_to_predict = \"Stock market surges today! Experts predict strong growth.\"\n",
    "prediction = agent5.predict(text_to_predict)\n",
    "print(f\"Prediction: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2469,
     "status": "ok",
     "timestamp": 1732273131520,
     "user": {
      "displayName": "VIRAJ D NAIK",
      "userId": "05442403411645043460"
     },
     "user_tz": -330
    },
    "id": "1bhB_pOoXuR3",
    "outputId": "34076650-9395-4e73-aa14-d00315224381"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: '/content/drive/MyDrive/fine_tuned_xlm_roberta'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\mas_env\\lib\\site-packages\\transformers\\utils\\hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\mas_env\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\mas_env\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/fine_tuned_xlm_roberta'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 117\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m real_prob, fake_prob\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Instantiate Agent5\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m agent5 \u001b[38;5;241m=\u001b[39m \u001b[43mAgent5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/drive/MyDrive/fine_tuned_xlm_roberta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Example news article snippet\u001b[39;00m\n\u001b[0;32m    120\u001b[0m news_article \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Prime Minister of India is Criminal\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[11], line 39\u001b[0m, in \u001b[0;36mAgent5.__init__\u001b[1;34m(self, model_path, max_length)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/fine_tuned_xlm_roberta\u001b[39m\u001b[38;5;124m'\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m=\u001b[39m max_length\n",
      "File \u001b[1;32mc:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\mas_env\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:857\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    856\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[0;32m    859\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\mas_env\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:689\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    686\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m    688\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 689\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\mas_env\\lib\\site-packages\\transformers\\utils\\hub.py:469\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 469\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    470\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    471\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[1;31mOSError\u001b[0m: Incorrect path_or_model_id: '/content/drive/MyDrive/fine_tuned_xlm_roberta'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "#updated agent 5\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F  # For softmax\n",
    "\n",
    "# Custom Dataset Class for loading the input text data\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Agent 5 Class with XLM-RoBERTa\n",
    "class Agent5:\n",
    "    def __init__(self, model_path='/content/drive/MyDrive/fine_tuned_xlm_roberta', max_length=256):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def train(self, train_texts, train_labels, batch_size=8, epochs=3):\n",
    "        # Convert text data into a Dataset\n",
    "        train_dataset = NewsDataset(train_texts, train_labels, self.tokenizer, self.max_length)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "        # Use the AdamW optimizer and cross-entropy loss\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # Move model to GPU if available\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Move data to the same device as model\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "\n",
    "                # Optimizer step\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{epochs} - Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "    def predict_with_percentage(self, text):\n",
    "        # Tokenize and prepare input text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "\n",
    "        # Move data to the same device as model\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        # Prediction\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Convert probabilities to percentage\n",
    "        fake_prob = probs[0][0].item() * 100  # Probability of being fake\n",
    "        real_prob = probs[0][1].item() * 100  # Probability of being real\n",
    "\n",
    "        return real_prob, fake_prob\n",
    "\n",
    "\n",
    "# Instantiate Agent5\n",
    "agent5 = Agent5(model_path='/content/drive/MyDrive/fine_tuned_xlm_roberta')\n",
    "\n",
    "# Example news article snippet\n",
    "news_article = \"The Prime Minister of India is Criminal\"\n",
    "\n",
    "# Predict with percentages\n",
    "real_prob, fake_prob = agent5.predict_with_percentage(news_article)\n",
    "\n",
    "# Print results\n",
    "print(f'Real News Probability: {real_prob:.2f}%')\n",
    "print(f'Fake News Probability: {fake_prob:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2G9VfTESZ80D"
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "class Agent7:\n",
    "    def _init_(self, bearer_token):\n",
    "        \"\"\"\n",
    "        Initializes Agent7 with the necessary tools for tweet verification.\n",
    "\n",
    "        :param bearer_token: Twitter API bearer token for authentication.\n",
    "        \"\"\"\n",
    "        self.tweepy = tweepy  # Store tweepy module as a class attribute\n",
    "        self.client = tweepy.Client(bearer_token=bearer_token)\n",
    "\n",
    "    def check_tweet_existence(self, query, max_results=10):\n",
    "        \"\"\"\n",
    "        Checks if any tweets exist for a given query.\n",
    "\n",
    "        :param query: Search query string (e.g., \"AI technology\").\n",
    "        :param max_results: Maximum number of tweets to fetch for verification.\n",
    "        :return: Boolean indicating if tweets exist, and the fetched tweets.\n",
    "        \"\"\"\n",
    "        search_query = f\"{query} -is:retweet lang:en\"  # Filter retweets, only English\n",
    "        try:\n",
    "            response = self.client.search_recent_tweets(query=search_query, max_results=max_results)\n",
    "            if response.data:\n",
    "                tweets = [tweet.text for tweet in response.data]\n",
    "                return 0, tweets  # Return 0 when tweets are found\n",
    "            else:\n",
    "                return 1, []  # Return 1 when no tweets are found\n",
    "        except self.tweepy.errors.TooManyRequests:\n",
    "            print(\"Error: Rate limit exceeded. Please try again later.\")\n",
    "            return 2, []  # Return 2 when rate limit error occurs\n",
    "        except self.tweepy.errors.Unauthorized as e:\n",
    "            print(\"Error: Unauthorized access. Check your bearer token.\")\n",
    "            return 1, []  # Return 1 for unauthorized access (no tweets)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error while checking tweet existence: {e}\")\n",
    "            return 1, []  # Return 1 for any other errors (no tweets)\n",
    "\n",
    "    def fetch_and_check(self, query, max_results=10):\n",
    "        \"\"\"\n",
    "        Combines fetching and verifying tweets for a given query.\n",
    "\n",
    "        :param query: Search query string.\n",
    "        :param max_results: Maximum number of tweets to fetch for verification.\n",
    "        :return: Return code and the fetched tweets.\n",
    "        \"\"\"\n",
    "        return self.check_tweet_existence(query, max_results)\n",
    "\n",
    "'''\n",
    "# Example Test\n",
    "if _name_ == \"_main_\":\n",
    "    # Replace with your Twitter API bearer token\n",
    "    bearer_token = \"AAAAAAAAAAAAAAAAAAAAADg%2FxAEAAAAASGOF%2Fy04UuLbMC4YH8ohnH6Or68%3DZtofEWGKh1WakS28MbJgrVs7vbx7p9jGuC9vNoKDEmcrIeQdWC\"\n",
    "    agent = Agent7(bearer_token)\n",
    "\n",
    "    # Test case with a single query\n",
    "    query = \"Destroy india\"\n",
    "    result_code, tweets = agent.fetch_and_check(query)\n",
    "\n",
    "    if result_code == 0:\n",
    "        print(f\"Tweets found for query '{query}': {tweets}\")\n",
    "    elif result_code == 1:\n",
    "        print(f\"No tweets found for query '{query}'.\")\n",
    "    elif result_code == 2:\n",
    "        print(f\"Rate limit exceeded for query '{query}'. Please try again later.\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1394,
     "status": "ok",
     "timestamp": 1732273601638,
     "user": {
      "displayName": "VIRAJ D NAIK",
      "userId": "05442403411645043460"
     },
     "user_tz": -330
    },
    "id": "BN_J35u5eMvA",
    "outputId": "9dfa1bfd-1e3b-4941-9399-b5c39ca67beb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Rate limit exceeded. Please try again later.\n",
      "Return Code: 2\n",
      "Fetched Tweets: []\n",
      "Scraping was not possible\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "class Agent7:\n",
    "    def __init__(self, bearer_token):\n",
    "        \"\"\"\n",
    "        Initializes Agent7 with the necessary tools for tweet verification.\n",
    "\n",
    "        :param bearer_token: Twitter API bearer token for authentication.\n",
    "        \"\"\"\n",
    "        self.tweepy = tweepy  # Store tweepy module as a class attribute\n",
    "        self.client = tweepy.Client(bearer_token=bearer_token)\n",
    "\n",
    "    def check_tweet_existence(self, query, max_results=10):\n",
    "        \"\"\"\n",
    "        Checks if any tweets exist for a given query.\n",
    "\n",
    "        :param query: Search query string (e.g., \"AI technology\").\n",
    "        :param max_results: Maximum number of tweets to fetch for verification.\n",
    "        :return: Return code and the fetched tweets.\n",
    "        \"\"\"\n",
    "        search_query = f\"{query} -is:retweet lang:en\"  # Filter retweets, only English\n",
    "        try:\n",
    "            response = self.client.search_recent_tweets(query=search_query, max_results=max_results)\n",
    "            if response.data:\n",
    "                tweets = [tweet.text for tweet in response.data]\n",
    "                return 0, tweets  # Return 0 when tweets are found\n",
    "            else:\n",
    "                return 1, []  # Return 1 when no tweets are found\n",
    "        except self.tweepy.errors.TooManyRequests:\n",
    "            print(\"Error: Rate limit exceeded. Please try again later.\")\n",
    "            return 2, []  # Return 2 when rate limit error occurs\n",
    "        except self.tweepy.errors.Unauthorized as e:\n",
    "            print(\"Error: Unauthorized access. Check your bearer token.\")\n",
    "            return 1, []  # Return 1 for unauthorized access (no tweets)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error while checking tweet existence: {e}\")\n",
    "            return 1, []  # Return 1 for any other errors (no tweets)\n",
    "\n",
    "    def fetch_and_check(self, query, max_results=10):\n",
    "        \"\"\"\n",
    "        Combines fetching and verifying tweets for a given query.\n",
    "\n",
    "        :param query: Search query string.\n",
    "        :param max_results: Maximum number of tweets to fetch for verification.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # Call the check_tweet_existence function\n",
    "        return_code, tweets = self.check_tweet_existence(query, max_results)\n",
    "\n",
    "        # Print the return values\n",
    "        print(f\"Return Code: {return_code}\")\n",
    "        print(f\"Fetched Tweets: {tweets}\")\n",
    "\n",
    "        # Allocate dynamic weight or handle scraping issue\n",
    "        if return_code == 0:\n",
    "            dynamic_weight = 10\n",
    "            print(f\"Dynamic Weight: {dynamic_weight} (Tweets found)\")\n",
    "        elif return_code == 1:\n",
    "            dynamic_weight = 0\n",
    "            print(f\"Dynamic Weight: {dynamic_weight} (No tweets found)\")\n",
    "        elif return_code == 2:\n",
    "            print(\"Scraping was not possible\")\n",
    "\n",
    "\n",
    "# Replace 'your_bearer_token_here' with your actual Twitter API bearer token.\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAADg%2FxAEAAAAASGOF%2Fy04UuLbMC4YH8ohnH6Or68%3DZtofEWGKh1WakS28MbJgrVs7vbx7p9jGuC9vNoKDEmcrIeQdWC'\n",
    "agent = Agent7(bearer_token)\n",
    "\n",
    "# Test the functionality\n",
    "query = \"Barbenheimer\"\n",
    "agent.fetch_and_check(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 511,
     "status": "ok",
     "timestamp": 1732279165430,
     "user": {
      "displayName": "VIRAJ D NAIK",
      "userId": "05442403411645043460"
     },
     "user_tz": -330
    },
    "id": "ykNQRP4ReYIl"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Agent1:\n",
    "    def __init__(self, case_weight=5, decay_rate=0.1):\n",
    "        self.case_weight = case_weight\n",
    "        self.decay_rate = decay_rate\n",
    "        self.special_symbols = \"!@#$%^&*()_+=-[]{}|;:'\\\",<>?/\\\\\"  # Special symbols\n",
    "\n",
    "    def check_special_symbols(self, text):\n",
    "        if not text:  # Handle empty text case\n",
    "            print(\"Empty text detected.\")\n",
    "            return 0  # No weight allocated for empty text\n",
    "\n",
    "        special_count = 0\n",
    "        total_count = len(text)  # Total number of characters in the text\n",
    "\n",
    "        for char in text:\n",
    "            if char in self.special_symbols:\n",
    "                special_count += 1\n",
    "\n",
    "        # Calculate the percentage of special symbols\n",
    "        percentage = (special_count / total_count) * 100\n",
    "\n",
    "        print(f\"Special Symbols: {special_count}, Total Characters: {total_count}, Percentage: {percentage:.2f}%\")\n",
    "\n",
    "        # Apply weight logic\n",
    "        if percentage <= 20:\n",
    "            weight = self.case_weight\n",
    "        else:\n",
    "            # Exponential decay for percentages above 20%\n",
    "            reduction_factor = math.exp(-self.decay_rate * (percentage - 20))\n",
    "            weight = self.case_weight * reduction_factor\n",
    "            print(f\"Reduction Factor: {reduction_factor:.4f}, Adjusted Weight: {weight:.2f}\")\n",
    "\n",
    "        return weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5708,
     "status": "ok",
     "timestamp": 1732279216282,
     "user": {
      "displayName": "VIRAJ D NAIK",
      "userId": "05442403411645043460"
     },
     "user_tz": -330
    },
    "id": "HJqHrxIf4lOj",
    "outputId": "b39965cb-de11-4103-9467-b2cd2aaf9ef1"
   },
   "outputs": [],
   "source": [
    "#!pip install vaderSentiment\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "class Agent2:\n",
    "    def __init__(self, case_weight=15):\n",
    "        self.case_weight = case_weight\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def analyze_sentiment(self, text):\n",
    "        if not text:  # Handle empty text case\n",
    "            print(\"Empty text detected.\")\n",
    "            return 0  # No sentiment detected for empty text\n",
    "\n",
    "        # Analyze the sentiment of the provided text\n",
    "        sentiment_score = self.analyzer.polarity_scores(text)\n",
    "        compound_score = sentiment_score['compound']\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate percentage based on compound score\n",
    "        if compound_score == 0:\n",
    "            # Neutral sentiment\n",
    "            sentiment_percentage = 50\n",
    "        elif compound_score > 0:\n",
    "            # Positive sentiment\n",
    "            sentiment_percentage = 50 + (compound_score * 50)  # Linear increase from 50% to 100%\n",
    "        else:\n",
    "            # Negative sentiment\n",
    "            sentiment_percentage = 50 + (compound_score * 50)  # Linear decrease from 50% to 0%\n",
    "\n",
    "        print(f\"Sentiment Percentage: {sentiment_percentage:.2f}%\")\n",
    "\n",
    "        # Final weight calculation based on sentiment percentage\n",
    "        final_weight = (self.case_weight * sentiment_percentage) / 100\n",
    "        return final_weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1732282856483,
     "user": {
      "displayName": "VIRAJ D NAIK",
      "userId": "05442403411645043460"
     },
     "user_tz": -330
    },
    "id": "xFX-zFHA4qLn"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Agent3:\n",
    "    def __init__(self, case_weight=5, threshold=10, decay_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize Agent3 with parameters for weight, threshold, and decay rate.\n",
    "\n",
    "        Args:\n",
    "            case_weight (float): The base weight to assign.\n",
    "            threshold (float): The percentage threshold for applying decay.\n",
    "            decay_rate (float): The rate of exponential decay for weights above the threshold.\n",
    "        \"\"\"\n",
    "        self.case_weight = case_weight\n",
    "        self.threshold = threshold\n",
    "        self.decay_rate = decay_rate\n",
    "\n",
    "    def count_hashtags(self, text):\n",
    "        \"\"\"\n",
    "        Count the percentage of words in the text that are hashtags and apply weight logic.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to analyze.\n",
    "\n",
    "        Returns:\n",
    "            float: The adjusted weight based on hashtag percentage and decay logic.\n",
    "        \"\"\"\n",
    "        if not text:  # Handle empty text case\n",
    "            print(\"Empty text detected.\")\n",
    "            return 0.0  # No hashtags detected for empty text\n",
    "\n",
    "        # Split the text into words to calculate the percentage of hashtags\n",
    "        words = text.split()\n",
    "        hashtag_count = sum(1 for word in words if word.startswith('#'))\n",
    "\n",
    "        # Calculate the percentage of hashtags relative to the total number of words\n",
    "        total_words = len(words)\n",
    "\n",
    "        if total_words == 0:  # Avoid division by zero\n",
    "            return 0.0\n",
    "\n",
    "        hashtag_percentage = (hashtag_count / total_words) * 100\n",
    "\n",
    "        print(f\"Hashtags: {hashtag_count}, Total Words: {total_words}, Percentage: {hashtag_percentage:.2f}%\")\n",
    "\n",
    "        # Apply weight logic based on the threshold\n",
    "        if hashtag_percentage <= self.threshold:\n",
    "            weight = self.case_weight\n",
    "            print(f\"Hashtag Percentage below threshold. Weight: {weight}\")\n",
    "        else:\n",
    "            # Exponential decay for percentages above the threshold\n",
    "            reduction_factor = math.exp(-self.decay_rate * (hashtag_percentage - self.threshold))\n",
    "            weight = self.case_weight * reduction_factor\n",
    "            print(f\"Reduction Factor: {reduction_factor:.4f}, Adjusted Weight: {weight:.2f}\")\n",
    "\n",
    "        return weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1732279249057,
     "user": {
      "displayName": "VIRAJ D NAIK",
      "userId": "05442403411645043460"
     },
     "user_tz": -330
    },
    "id": "7izM4shO43K5"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "class Agent4:\n",
    "    def __init__(self, case_weight, decay_rate=0.1, threshold=10):\n",
    "        self.case_weight = case_weight  # Base weight for the agent\n",
    "        self.decay_rate = decay_rate   # How quickly the weight reduces above the threshold\n",
    "        self.threshold = threshold    # Percentage threshold for decay application\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "        self.clickbait_keywords = [\n",
    "            \"shocking\", \"you won’t believe\", \"what happened next\", \"this is why\",\n",
    "            \"never seen before\", \"mind-blowing\", \"amazing\", \"unbelievable\",\n",
    "            \"click here\", \"must read\", \"you'll regret not\", \"read now\", \"only\",\n",
    "            \"warning\", \"secret\", \"revealed\", \"discovered\"\n",
    "        ]\n",
    "\n",
    "    def check_clickbait(self, text, headline=None, body=None):\n",
    "        if not text:  # Handle empty text case\n",
    "            print(\"Empty text detected.\")\n",
    "            return 0  # No weight allocated for empty text\n",
    "\n",
    "        # Lowercase for uniformity\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        # Check for clickbait keywords\n",
    "        keyword_count = sum(1 for keyword in self.clickbait_keywords if keyword in text_lower)\n",
    "\n",
    "        # Check for excessive punctuation\n",
    "        exclamation_marks = len(re.findall(r'!', text))\n",
    "        question_marks = len(re.findall(r'\\?', text))\n",
    "        punctuation_score = 1 if exclamation_marks > 2 or question_marks > 1 else 0\n",
    "\n",
    "        # Check sentiment score\n",
    "        sentiment_score = self.analyzer.polarity_scores(text)['compound']\n",
    "        sentiment_clickbait = 1 if sentiment_score <= -0.5 else 0\n",
    "\n",
    "        # Optional: Compare headline and body\n",
    "        headline_body_match = 0\n",
    "        if headline and body:\n",
    "            body_start = body[:100].lower()\n",
    "            if headline.lower() in body_start:\n",
    "                headline_body_match = 1\n",
    "\n",
    "        # Combine results into a \"clickbait score\"\n",
    "        clickbait_score = keyword_count + punctuation_score + sentiment_clickbait + headline_body_match\n",
    "\n",
    "        # Set theoretical maximum score (4, if all factors are contributing)\n",
    "        max_possible_score = 4  # Maximum score for the clickbait components considered\n",
    "\n",
    "        # Calculate clickbait percentage\n",
    "        if max_possible_score == 0:  # Edge case if max_possible_score is zero\n",
    "            clickbait_percentage = 0\n",
    "        else:\n",
    "            clickbait_percentage = (clickbait_score / max_possible_score) * 100\n",
    "\n",
    "        print(f\"Clickbait Score: {clickbait_score}, Percentage: {clickbait_percentage:.2f}%\")\n",
    "\n",
    "        # Apply weight logic based on clickbait percentage\n",
    "        if clickbait_percentage <= self.threshold:\n",
    "            weight = self.case_weight\n",
    "        else:\n",
    "            # Apply exponential decay for percentages above the threshold\n",
    "            reduction_factor = math.exp(-self.decay_rate * (clickbait_percentage - self.threshold))\n",
    "            weight = self.case_weight * reduction_factor\n",
    "            print(f\"Reduction Factor: {reduction_factor:.4f}, Adjusted Weight: {weight:.2f}\")\n",
    "\n",
    "        return weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1732280641324,
     "user": {
      "displayName": "VIRAJ D NAIK",
      "userId": "05442403411645043460"
     },
     "user_tz": -330
    },
    "id": "AGK5Qopr46SE"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Agent5:\n",
    "    def __init__(self, model_path=r'C:\\Users\\viraj\\Documents\\Virajs Projects\\Fake News Detection using MAS\\fine_tuned_xlm_roberta', max_length=256, true_weight=45):\n",
    "        # Load the pre-trained tokenizer and model from the given path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        self.max_length = max_length\n",
    "        self.true_weight = true_weight  # Assign only the true weight during initialization\n",
    "\n",
    "    def train(self, train_texts, train_labels, batch_size=8, epochs=3):\n",
    "        # Convert text data into a Dataset\n",
    "        train_dataset = NewsDataset(train_texts, train_labels, self.tokenizer, self.max_length)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "        # Use the AdamW optimizer and cross-entropy loss\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # Move model to GPU if available\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Move data to the same device as model\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "\n",
    "                # Optimizer step\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{epochs} - Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "    def predict_true_weight(self, text):\n",
    "        # Tokenize and prepare input text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "\n",
    "        # Move data to the same device as model\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        # Prediction\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Extract true news probability and calculate weight\n",
    "        true_prob = probs[0][1].item() * 100  # Probability of being true\n",
    "        true_weighted = true_prob * self.true_weight / 100  # Weighted true value\n",
    "\n",
    "        return true_weighted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 709,
     "status": "ok",
     "timestamp": 1732279293209,
     "user": {
      "displayName": "VIRAJ D NAIK",
      "userId": "05442403411645043460"
     },
     "user_tz": -330
    },
    "id": "EROQwt7p4_oC"
   },
   "outputs": [],
   "source": [
    "class Agent7:\n",
    "    def __init__(self, bearer_token):  # Corrected __init__ (double underscores)\n",
    "        \"\"\"\n",
    "        Initializes Agent7 with the necessary tools for tweet verification.\n",
    "\n",
    "        :param bearer_token: Twitter API bearer token for authentication.\n",
    "        \"\"\"\n",
    "        self.tweepy = tweepy  # Store tweepy module as a class attribute\n",
    "        self.client = tweepy.Client(bearer_token=bearer_token)\n",
    "\n",
    "    def check_tweet_existence(self, query, max_results=10):\n",
    "        \"\"\"\n",
    "        Checks if any tweets exist for a given query.\n",
    "\n",
    "        :param query: Search query string (e.g., \"AI technology\").\n",
    "        :param max_results: Maximum number of tweets to fetch for verification.\n",
    "        :return: Return code and the fetched tweets.\n",
    "        \"\"\"\n",
    "        search_query = f\"{query} -is:retweet lang:en\"  # Filter retweets, only English\n",
    "        try:\n",
    "            response = self.client.search_recent_tweets(query=search_query, max_results=max_results)\n",
    "            if response.data:\n",
    "                tweets = [tweet.text for tweet in response.data]\n",
    "                return 0, tweets  # Return 0 when tweets are found\n",
    "            else:\n",
    "                return 1, []  # Return 1 when no tweets are found\n",
    "        except self.tweepy.errors.TooManyRequests:\n",
    "            print(\"Error: Rate limit exceeded. Please try again later.\")\n",
    "            return 2, []  # Return 2 when rate limit error occurs\n",
    "        except self.tweepy.errors.Unauthorized:\n",
    "            print(\"Error: Unauthorized access. Check your bearer token.\")\n",
    "            return 1, []  # Return 1 for unauthorized access (no tweets)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error while checking tweet existence: {e}\")\n",
    "            return 1, []  # Return 1 for any other errors (no tweets)\n",
    "\n",
    "    def fetch_and_check(self, query, max_results=10):\n",
    "        \"\"\"\n",
    "        Combines fetching and verifying tweets for a given query.\n",
    "\n",
    "        :param query: Search query string.\n",
    "        :param max_results: Maximum number of tweets to fetch for verification.\n",
    "        :return: Weight based on tweet existence or \"Not Working\" in case of error, and the tweets if any.\n",
    "        \"\"\"\n",
    "        # Call the check_tweet_existence function\n",
    "        return_code, tweets = self.check_tweet_existence(query, max_results)\n",
    "\n",
    "        # Allocate dynamic weight or handle scraping issue\n",
    "        if return_code == 0:\n",
    "            return 10, tweets  # Return 10 when tweets are found\n",
    "        elif return_code == 1:\n",
    "            return 0, []  # Return 0 when no tweets are found\n",
    "        elif return_code == 2:\n",
    "            return \"Not Working\", []  # Return \"Not Working\" for rate limit or errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1732280645421,
     "user": {
      "displayName": "VIRAJ D NAIK",
      "userId": "05442403411645043460"
     },
     "user_tz": -330
    },
    "id": "QExexxYr5E_X"
   },
   "outputs": [],
   "source": [
    "class Agent6Integration:\n",
    "    def __init__(self, agent1, agent2, agent3, agent4, agent5, agent7):  # Correct constructor name\n",
    "        self.agent1 = agent1\n",
    "        self.agent2 = agent2\n",
    "        self.agent3 = agent3\n",
    "        self.agent4 = agent4\n",
    "        self.agent5 = agent5\n",
    "        self.agent7 = agent7\n",
    "\n",
    "    def run_agents(self, text, query=None, max_results=10, headline=None, body=None):\n",
    "        # Run Agent 1 (Special symbols check)\n",
    "        special_symbols_percent = self.agent1.check_special_symbols(text)\n",
    "\n",
    "        # Run Agent 2 (Sentiment analysis)\n",
    "        sentiment = self.agent2.analyze_sentiment(text)\n",
    "\n",
    "        # Run Agent 3 (Hashtags count)\n",
    "        hashtags_percent = self.agent3.count_hashtags(text)\n",
    "\n",
    "        # Run Agent 4 (Clickbait detection)\n",
    "        clickbait = self.agent4.check_clickbait(text, headline, body)\n",
    "\n",
    "        # Run Agent 5 (Fake news prediction using BERT)\n",
    "        real_prob, fake_prob = self.agent5.predict_with_percentage(text)\n",
    "\n",
    "        # Run Agent 7 (Tweet existence check)\n",
    "        tweet_result_code, tweets = self.agent7.fetch_and_check(query, max_results)\n",
    "\n",
    "        # Create the standardized output dictionary\n",
    "        standardized_outputs = {\n",
    "            \"Agent1_Percentage\": special_symbols_percent,\n",
    "            \"Agent2_Binary\": 1 if sentiment == 1 else 0,\n",
    "            \"Agent3_Percentage\": hashtags_percent,\n",
    "            \"Agent4_Binary\": 1 if clickbait else 0,\n",
    "            \"Agent5_Percentage\": fake_prob,\n",
    "            \"Agent7_Categorical\": tweet_result_code,\n",
    "        }\n",
    "\n",
    "        # Create and use Agent6 for applying weights\n",
    "        agent6 = Agent6()\n",
    "        weighted_score = agent6.apply_weights(standardized_outputs)\n",
    "\n",
    "        return weighted_score\n",
    "\n",
    "\n",
    "# Agent6 - Apply Weights Logic (with explanation)\n",
    "class Agent6:\n",
    "    def __init__(self):  # Correct constructor name\n",
    "        pass\n",
    "\n",
    "    def apply_weights(self, standardized_outputs):\n",
    "        # Retrieve the output from Agent 7\n",
    "        agent7_output = standardized_outputs[\"Agent7_Categorical\"]\n",
    "\n",
    "        # Define weights based on Agent7 output (if Twitter query results in issues)\n",
    "        if agent7_output == 2:  # Rate limit exceeded\n",
    "            weights = {\n",
    "                \"Agent1\": 5,\n",
    "                \"Agent2\": 15,\n",
    "                \"Agent3\": 5,\n",
    "                \"Agent4\": 25,\n",
    "                \"Agent5\": 45,\n",
    "                \"Agent7\": 0,\n",
    "            }\n",
    "        else:\n",
    "            # Assign normal weights if no issues with Agent7\n",
    "            weights = {\n",
    "                \"Agent1\": 5,\n",
    "                \"Agent2\": 15,\n",
    "                \"Agent3\": 5,\n",
    "                \"Agent4\": 20,\n",
    "                \"Agent5\": 40,\n",
    "                \"Agent7\": 10,\n",
    "            }\n",
    "\n",
    "        # Apply weights and calculate the final weighted score\n",
    "        adjusted_weights = {}\n",
    "        total_weight_contribution = 0\n",
    "        print(\"Individual Weight Contributions:\")\n",
    "\n",
    "        for agent, weight in weights.items():\n",
    "            if agent == \"Agent1\":\n",
    "                contribution = weight * (standardized_outputs[\"Agent1_Percentage\"] / 100)\n",
    "                adjusted_weights[agent] = contribution\n",
    "            elif agent == \"Agent2\":\n",
    "                contribution = weight * standardized_outputs[\"Agent2_Binary\"]\n",
    "                adjusted_weights[agent] = contribution\n",
    "            elif agent == \"Agent3\":\n",
    "                contribution = weight * (standardized_outputs[\"Agent3_Percentage\"] / 100)\n",
    "                adjusted_weights[agent] = contribution\n",
    "            elif agent == \"Agent4\":\n",
    "                contribution = weight * (1 - standardized_outputs[\"Agent4_Binary\"])\n",
    "                adjusted_weights[agent] = contribution\n",
    "            elif agent == \"Agent5\":\n",
    "                contribution = weight * (standardized_outputs[\"Agent5_Percentage\"] / 100)\n",
    "                adjusted_weights[agent] = contribution\n",
    "            elif agent == \"Agent7\":\n",
    "                contribution = weight * (1 if agent7_output in [0, 1] else 0)\n",
    "                adjusted_weights[agent] = contribution\n",
    "\n",
    "            # Add contribution to total weight\n",
    "            total_weight_contribution += contribution\n",
    "\n",
    "            # Print each agent's weight contribution\n",
    "            print(f\"{agent} Weight Contribution: {contribution:.2f}\")\n",
    "\n",
    "        # Display total weight before final weighted score\n",
    "        print(f\"\\nTotal Weight Contribution from all Agents: {total_weight_contribution:.2f}\")\n",
    "\n",
    "        # Calculate the final weighted score\n",
    "        print(f\"Final Weighted Score: {total_weight_contribution:.2f}\")\n",
    "        return total_weight_contribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19931,
     "status": "ok",
     "timestamp": 1732282887743,
     "user": {
      "displayName": "VIRAJ D NAIK",
      "userId": "05442403411645043460"
     },
     "user_tz": -330
    },
    "id": "mbob1Ff05Mgj",
    "outputId": "50e82779-8892-4bcb-f7d4-1d482368c3e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Agents...\n",
      "Special Symbols: 7, Total Characters: 436, Percentage: 1.61%\n",
      "Agent 1 Output: 5\n",
      "Sentiment Percentage: 29.91%\n",
      "Agent 2 Output (Sentiment): 4.48575\n",
      "Hashtags: 0, Total Words: 69, Percentage: 0.00%\n",
      "Hashtag Percentage below threshold. Weight: 5\n",
      "Agent 3 Output (Hashtags Percentage): 5\n",
      "Clickbait Score: 1, Percentage: 25.00%\n",
      "Reduction Factor: 0.2231, Adjusted Weight: 4.46\n",
      "Agent 4 Output (Clickbait): 4.462603202968596\n",
      "Agent 5 (Fake News Prediction): Real Probability: 44.999640583992004%, Fake Probability: 55.000359416007996%\n",
      "Error: Rate limit exceeded. Please try again later.\n",
      "Agent 7 Output (Tweet Result Code): Not Working, Tweets: []\n",
      "Applying Weights...\n",
      "Individual Weight Contributions:\n",
      "Agent1 Weight Contribution: 0.25\n",
      "Agent2 Weight Contribution: 0.00\n",
      "Agent3 Weight Contribution: 0.25\n",
      "Agent4 Weight Contribution: 0.00\n",
      "Agent5 Weight Contribution: 18.00\n",
      "Agent7 Weight Contribution: 0.00\n",
      "\n",
      "Total Weight Contribution from all Agents: 18.50\n",
      "Final Weighted Score: 18.50\n",
      "Final Weighted Score: 18.50\n"
     ]
    }
   ],
   "source": [
    "class Agent6Integration:\n",
    "    def __init__(self, agent1, agent2, agent3, agent4, agent5, agent7):  # Correct constructor name\n",
    "        self.agent1 = agent1\n",
    "        self.agent2 = agent2\n",
    "        self.agent3 = agent3\n",
    "        self.agent4 = agent4\n",
    "        self.agent5 = agent5\n",
    "        self.agent7 = agent7\n",
    "\n",
    "    def run_agents(self, text, query=None, max_results=10, headline=None, body=None):\n",
    "        print(\"Running Agents...\")  # Debugging statement\n",
    "\n",
    "        # Run Agent 1 (Special symbols check)\n",
    "        special_symbols_percent = self.agent1.check_special_symbols(text)\n",
    "        print(f\"Agent 1 Output: {special_symbols_percent}\")  # Debugging statement\n",
    "\n",
    "        # Run Agent 2 (Sentiment analysis)\n",
    "        sentiment = self.agent2.analyze_sentiment(text)\n",
    "        print(f\"Agent 2 Output (Sentiment): {sentiment}\")  # Debugging statement\n",
    "\n",
    "        # Run Agent 3 (Hashtags count)\n",
    "        hashtags_percent = self.agent3.count_hashtags(text)\n",
    "        print(f\"Agent 3 Output (Hashtags Percentage): {hashtags_percent}\")  # Debugging statement\n",
    "\n",
    "        # Run Agent 4 (Clickbait detection)\n",
    "        clickbait = self.agent4.check_clickbait(text, headline, body)\n",
    "        print(f\"Agent 4 Output (Clickbait): {clickbait}\")  # Debugging statement\n",
    "\n",
    "        # Run Agent 5 (Fake news prediction using BERT)\n",
    "        real_prob = self.agent5.predict_true_weight(text)\n",
    "        fake_prob = 100 - real_prob  # Debugging statement\n",
    "        print(f\"Agent 5 (Fake News Prediction): Real Probability: {real_prob}%, Fake Probability: {fake_prob}%\")\n",
    "\n",
    "        # Run Agent 7 (Tweet existence check)\n",
    "        tweet_result_code, tweets = self.agent7.fetch_and_check(query, max_results)\n",
    "        print(f\"Agent 7 Output (Tweet Result Code): {tweet_result_code}, Tweets: {tweets}\")  # Debugging statement\n",
    "\n",
    "        # Create the standardized output dictionary\n",
    "        standardized_outputs = {\n",
    "            \"Agent1_Percentage\": special_symbols_percent,\n",
    "            \"Agent2_Binary\": 1 if sentiment == 1 else 0,\n",
    "            \"Agent3_Percentage\": hashtags_percent,\n",
    "            \"Agent4_Binary\": 1 if clickbait else 0,\n",
    "            \"Agent5_Percentage\": real_prob,\n",
    "            \"Agent7_Categorical\": tweet_result_code,\n",
    "        }\n",
    "\n",
    "        # Now use Agent6 to apply weights\n",
    "        agent6 = Agent6()\n",
    "        weighted_score = agent6.apply_weights(standardized_outputs)\n",
    "\n",
    "        return weighted_score\n",
    "\n",
    "\n",
    "class Agent6:\n",
    "    def __init__(self):  # Correct constructor name\n",
    "        pass\n",
    "\n",
    "    def apply_weights(self, standardized_outputs):\n",
    "        print(\"Applying Weights...\")  # Debugging statement\n",
    "        # Retrieve the output from Agent 7\n",
    "        agent7_output = standardized_outputs[\"Agent7_Categorical\"]\n",
    "\n",
    "        # Define weights based on Agent7 output (if Twitter query results in issues)\n",
    "        if agent7_output == 2:  # Rate limit exceeded\n",
    "            weights = {\n",
    "                \"Agent1\": 5,\n",
    "                \"Agent2\": 15,\n",
    "                \"Agent3\": 5,\n",
    "                \"Agent4\": 25,\n",
    "                \"Agent5\": 45,\n",
    "                \"Agent7\": 0,\n",
    "            }\n",
    "        else:\n",
    "            # Assign normal weights if no issues with Agent7\n",
    "            weights = {\n",
    "                \"Agent1\": 5,\n",
    "                \"Agent2\": 15,\n",
    "                \"Agent3\": 5,\n",
    "                \"Agent4\": 20,\n",
    "                \"Agent5\": 40,\n",
    "                \"Agent7\": 10,\n",
    "            }\n",
    "\n",
    "        # Apply weights and calculate the final weighted score\n",
    "        adjusted_weights = {}\n",
    "        total_weight_contribution = 0\n",
    "        print(\"Individual Weight Contributions:\")\n",
    "\n",
    "        for agent, weight in weights.items():\n",
    "            if agent == \"Agent1\":\n",
    "                contribution = weight * (standardized_outputs[\"Agent1_Percentage\"] / 100)\n",
    "                adjusted_weights[agent] = contribution\n",
    "            elif agent == \"Agent2\":\n",
    "                contribution = weight * standardized_outputs[\"Agent2_Binary\"]\n",
    "                adjusted_weights[agent] = contribution\n",
    "            elif agent == \"Agent3\":\n",
    "                contribution = weight * (standardized_outputs[\"Agent3_Percentage\"] / 100)\n",
    "                adjusted_weights[agent] = contribution\n",
    "            elif agent == \"Agent4\":\n",
    "                contribution = weight * (1 - standardized_outputs[\"Agent4_Binary\"])\n",
    "                adjusted_weights[agent] = contribution\n",
    "            elif agent == \"Agent5\":\n",
    "                contribution = weight * (standardized_outputs[\"Agent5_Percentage\"] / 100)\n",
    "                adjusted_weights[agent] = contribution\n",
    "            elif agent == \"Agent7\":\n",
    "                contribution = weight * (1 if agent7_output in [0, 1] else 0)\n",
    "                adjusted_weights[agent] = contribution\n",
    "\n",
    "            # Add contribution to total weight\n",
    "            total_weight_contribution += contribution\n",
    "\n",
    "            # Print each agent's weight contribution\n",
    "            print(f\"{agent} Weight Contribution: {contribution:.2f}\")\n",
    "\n",
    "        # Display total weight before final weighted score\n",
    "        print(f\"\\nTotal Weight Contribution from all Agents: {total_weight_contribution:.2f}\")\n",
    "\n",
    "        # Calculate the final weighted score\n",
    "        print(f\"Final Weighted Score: {total_weight_contribution:.2f}\")\n",
    "        return total_weight_contribution\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize agents (you'll need to replace these with real implementations or mocks)\n",
    "    agent1 = Agent1()  # Assume you have the Agent1 class already\n",
    "    agent2 = Agent2()  # Assume you have the Agent2 class already\n",
    "    agent3 = Agent3()  # Assume you have the Agent3 class already\n",
    "    agent4 = Agent4(20)  # Assume you have the Agent4 class already\n",
    "    agent5 = Agent5()  # Assume you have the Agent5 class already\n",
    "    agent7 = Agent7(bearer_token=\"AAAAAAAAAAAAAAAAAAAAAD8%2BxAEAAAAA4mFW4oKeg5L5Je9VJyQUnLsm28M%3DCYuxaWZVBhKMGthEBSFoCXvk0VLhGPHEqTJIF9q3YCAB1M3eGr\")  # Your actual token\n",
    "\n",
    "    # Integrate agents and Agent6\n",
    "    agent6_integration = Agent6Integration(agent1, agent2, agent3, agent4, agent5, agent7)\n",
    "\n",
    "    # Test sentence for classification (user input)\n",
    "    text = input(\"Please enter a sentence for classification: \")\n",
    "\n",
    "    # Optionally, provide additional parameters (query, headline, body)\n",
    "    query = text\n",
    "    headline = \"The shocking truth about AI\"\n",
    "    body = \"This is a must-read article that unveils the shocking truth about AI and technology.\"\n",
    "\n",
    "    # Run all agents and get the final weighted score\n",
    "    weighted_score = agent6_integration.run_agents(text, query, max_results=10, headline=headline, body=body)\n",
    "    print(f\"Final Weighted Score: {weighted_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnWBlpdH9Yo2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP7r4V6E3E5dfvSieQjwwlt",
   "gpuType": "T4",
   "mount_file_id": "1FsGowNvF9uSlSaInDzd89kGaApOpgjP-",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mas_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01b0c8aacead44d08f705bb72df90c06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "05262276ea9049c595136ed72798aaf6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b3f51b2756c42c1940f01c92764d2b0",
      "placeholder": "​",
      "style": "IPY_MODEL_4627fb01ec3b49ff83cb193d15928ab5",
      "value": "config.json: 100%"
     }
    },
    "09fcf47001f5466591ff6bdbcd5eeeb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0b2e2e8ccd53448fb72215def338fa34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0c293fc53f1a498ea4a1056d6ef52194": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd53f8d132464f1db09529ecd2e00a3a",
      "placeholder": "​",
      "style": "IPY_MODEL_1d09ef0161ed43b7bcc79a44a56e5e98",
      "value": " 615/615 [00:00&lt;00:00, 25.9kB/s]"
     }
    },
    "1d09ef0161ed43b7bcc79a44a56e5e98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "20e82ce495e14c2493f25a44785f5c45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74084694b54d4329a53b02864e47ae64",
      "max": 615,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ea9c0b763e214b27912f4134a6e774a8",
      "value": 615
     }
    },
    "347b4a1815fb4d7eb615ae589d99a79f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8fb3cec2b0349ce88efa0674c36ccd1",
      "max": 25,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_01b0c8aacead44d08f705bb72df90c06",
      "value": 25
     }
    },
    "37415394f753457285d2cbeb11c18b27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d49c6409ce04754bf7edaf228d15ed9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b316fd403e52433cb347ab9ed64c3ac7",
       "IPY_MODEL_b86fda9a73694286a1df15afeb95129b",
       "IPY_MODEL_f62b91c58f924015bb84dd9912242a8f"
      ],
      "layout": "IPY_MODEL_757805568b3c40da83aeef37b72f0f3a"
     }
    },
    "41ec069c61c94a6798bb61e85816d1a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "426c4ff9417743d5b3cef2627ac66f51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "452a570a5681462b99f0fbe1f1698ffb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4627fb01ec3b49ff83cb193d15928ab5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "46b5471f23134224bded2854623fa309": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b1ce1baa6834c4bbb43fbefa562cf26",
      "placeholder": "​",
      "style": "IPY_MODEL_ec316d0f17c2458599af6b7c26e67606",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "46ba3c4b2457480a98ad7bdc7e408b0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec74ba0c59da483a8f3168ac372617ca",
      "max": 9096718,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_09fcf47001f5466591ff6bdbcd5eeeb0",
      "value": 9096718
     }
    },
    "4c6bce7efb03445d8fdb0b8db7cd0576": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6701a3dbdb34e38bb2f4749f90ad267",
      "max": 1115567652,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ce702fccfa454da9aa0f5208f3782b8e",
      "value": 1115567652
     }
    },
    "4db5c648c4dd46528203c560d5152c96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b3f51b2756c42c1940f01c92764d2b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6265527751fa4b2c811f27a367349758": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81e276447034495eab56dd8831389e30",
      "placeholder": "​",
      "style": "IPY_MODEL_37415394f753457285d2cbeb11c18b27",
      "value": "model.safetensors: 100%"
     }
    },
    "74084694b54d4329a53b02864e47ae64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "757805568b3c40da83aeef37b72f0f3a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81e276447034495eab56dd8831389e30": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "868a9029bfc44520af9c184b46e25d41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d377aeb89bf44900bb02749f755377ca",
      "placeholder": "​",
      "style": "IPY_MODEL_b98a4364a965460cbaa9481042f72548",
      "value": "tokenizer.json: 100%"
     }
    },
    "8a88f40f7f274bdb8a48878d546aa1bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b1ce1baa6834c4bbb43fbefa562cf26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c2ac2adc3fe4b69af9dd973b79b3531": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d5064ceee164e70a0c4747db9c4073f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_942c1ac075794068b907935e95d86d7a",
      "placeholder": "​",
      "style": "IPY_MODEL_0b2e2e8ccd53448fb72215def338fa34",
      "value": " 9.10M/9.10M [00:00&lt;00:00, 17.1MB/s]"
     }
    },
    "942c1ac075794068b907935e95d86d7a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f528776008349548cc089296519e8f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e13bb378cfcb46cb8596a30b570dd6ff",
      "placeholder": "​",
      "style": "IPY_MODEL_8a88f40f7f274bdb8a48878d546aa1bc",
      "value": " 25.0/25.0 [00:00&lt;00:00, 1.61kB/s]"
     }
    },
    "a9b97cfc7aec499b9c7938708aef5ced": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_46b5471f23134224bded2854623fa309",
       "IPY_MODEL_347b4a1815fb4d7eb615ae589d99a79f",
       "IPY_MODEL_9f528776008349548cc089296519e8f3"
      ],
      "layout": "IPY_MODEL_e2540bb6c983416699b938ed0d90b8a3"
     }
    },
    "aa2371e735a540b2890e2df7900a515f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b30949ba3fb1413d9a7dce610cfd586e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b316fd403e52433cb347ab9ed64c3ac7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_452a570a5681462b99f0fbe1f1698ffb",
      "placeholder": "​",
      "style": "IPY_MODEL_426c4ff9417743d5b3cef2627ac66f51",
      "value": "sentencepiece.bpe.model: 100%"
     }
    },
    "b3f22b02c4e14846838ee002491f8676": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b86fda9a73694286a1df15afeb95129b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8aa1a2a90d042b6a08db1c097e6b0b1",
      "max": 5069051,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_41ec069c61c94a6798bb61e85816d1a9",
      "value": 5069051
     }
    },
    "b98a4364a965460cbaa9481042f72548": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba4b11280c7f4b68ac1b8076999bd932": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4db5c648c4dd46528203c560d5152c96",
      "placeholder": "​",
      "style": "IPY_MODEL_8c2ac2adc3fe4b69af9dd973b79b3531",
      "value": " 1.12G/1.12G [00:08&lt;00:00, 61.9MB/s]"
     }
    },
    "c303a708f096419b8d0042356f35d1c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_868a9029bfc44520af9c184b46e25d41",
       "IPY_MODEL_46ba3c4b2457480a98ad7bdc7e408b0e",
       "IPY_MODEL_8d5064ceee164e70a0c4747db9c4073f"
      ],
      "layout": "IPY_MODEL_b3f22b02c4e14846838ee002491f8676"
     }
    },
    "c371f65583c3476cb789c6d041350b82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_05262276ea9049c595136ed72798aaf6",
       "IPY_MODEL_20e82ce495e14c2493f25a44785f5c45",
       "IPY_MODEL_0c293fc53f1a498ea4a1056d6ef52194"
      ],
      "layout": "IPY_MODEL_d86e662046154edf808f1a50664e6aac"
     }
    },
    "c6f9831667fa4aab8c5aae2ca4704c16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8fb3cec2b0349ce88efa0674c36ccd1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce493ed4ee7c4fb89ea381f4f9bd02d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6265527751fa4b2c811f27a367349758",
       "IPY_MODEL_4c6bce7efb03445d8fdb0b8db7cd0576",
       "IPY_MODEL_ba4b11280c7f4b68ac1b8076999bd932"
      ],
      "layout": "IPY_MODEL_aa2371e735a540b2890e2df7900a515f"
     }
    },
    "ce702fccfa454da9aa0f5208f3782b8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d377aeb89bf44900bb02749f755377ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d86e662046154edf808f1a50664e6aac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd53f8d132464f1db09529ecd2e00a3a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e13bb378cfcb46cb8596a30b570dd6ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2540bb6c983416699b938ed0d90b8a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8aa1a2a90d042b6a08db1c097e6b0b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea9c0b763e214b27912f4134a6e774a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ec316d0f17c2458599af6b7c26e67606": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec74ba0c59da483a8f3168ac372617ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f62b91c58f924015bb84dd9912242a8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b30949ba3fb1413d9a7dce610cfd586e",
      "placeholder": "​",
      "style": "IPY_MODEL_c6f9831667fa4aab8c5aae2ca4704c16",
      "value": " 5.07M/5.07M [00:00&lt;00:00, 10.9MB/s]"
     }
    },
    "f6701a3dbdb34e38bb2f4749f90ad267": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
